{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1l8ROqTZFwESdaYtjdWefCdUgXrTef0bI",
      "authorship_tag": "ABX9TyO4jV+SYaS/0fL6qjpm52ab",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pradosh2008/spark-playground/blob/main/hellospark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Steps to run pyspark with logging in colab"
      ],
      "metadata": {
        "id": "jyCSPkaI10Yw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install the necessary package\n"
      ],
      "metadata": {
        "id": "73M_URG54ajz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JE-S8-BVjxLn"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://downloads.apache.org/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz\n"
      ],
      "metadata": {
        "id": "ESKbjZelk2ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh /content/spark-3.5.3-bin-hadoop3.tgz\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_LwSsk9lH5C",
        "outputId": "e902bb3d-fdcf-49b7-b41d-243beac9242c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 21K Oct  9 09:16 /content/spark-3.5.3-bin-hadoop3.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!file /content/spark-3.5.3-bin-hadoop3.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GPPlI6qpy0V",
        "outputId": "9be919ae-4c40-4884-cdce-5fe02ed9125d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/spark-3.5.3-bin-hadoop3.tgz: gzip compressed data, from Unix, original size modulo 2^32 445972480\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzf spark-3.5.3-bin-hadoop3.tgz\n"
      ],
      "metadata": {
        "id": "68k5rbsuk6Bk",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set the path variable"
      ],
      "metadata": {
        "id": "65v8y5Eo4uAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import findspark\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.3-bin-hadoop3\"\n",
        "\n",
        "findspark.init()\n"
      ],
      "metadata": {
        "id": "gZWiKZscj5rE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# create a log4j.properties file"
      ],
      "metadata": {
        "id": "Be7W0LcE4xFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#put it inside conf folder  /content/spark-3.5.3-bin-hadoop3/conf/log4j.properties\n",
        "\n",
        "#This is a basic temp_pyfile\n",
        "\n",
        "# log4j_properties = \"\"\"\n",
        "# log4j.rootCategory=INFO, console\n",
        "# log4j.appender.console=org.apache.log4j.ConsoleAppender\n",
        "# log4j.appender.console.target=System.err\n",
        "# log4j.appender.console.layout=org.apache.log4j.PatternLayout\n",
        "# log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n",
        "# \"\"\"\n",
        "\n",
        "# with open(\"/content/spark-3.5.3-bin-hadoop3/conf/log4j.properties\", \"w\") as f:\n",
        "#     f.write(log4j_properties)"
      ],
      "metadata": {
        "id": "P2-cLxlErUeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the spark program"
      ],
      "metadata": {
        "id": "Z1b5xKvG44h5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "#from pyspark import SparkContext\n",
        "#from pyspark.sql import SQLContext\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"ColabSparkLog4j\").getOrCreate()\n",
        "\n",
        "# Set up logger\n",
        "log4jLogger = spark._jvm.org.apache.log4j\n",
        "logger = log4jLogger.LogManager.getLogger(__name__)\n",
        "\n",
        "# Configure logger to write to a log file\n",
        "log_file_path = \"/content/drive/MyDrive/spark-playground/HelloSpark/app-logs/spark_log4j_output.log\"\n",
        "log4jLogger.BasicConfigurator.configure(log4jLogger.FileAppender(log4jLogger.PatternLayout(\"%d %-5p [%c{1}] %m%n\"), log_file_path))\n",
        "\n",
        "logger.info(\"Spark application is starting...\")\n",
        "\n",
        "# Example of a simple Spark application\n",
        "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Catherine\", 29)]\n",
        "columns = [\"Name\", \"Age\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n",
        "\n",
        "logger.info(\"Spark application completed.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_CkwlmKsBs8",
        "outputId": "8a723657-18fe-40b7-f513-649571de2be4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+\n",
            "|     Name|Age|\n",
            "+---------+---+\n",
            "|    Alice| 34|\n",
            "|      Bob| 45|\n",
            "|Catherine| 29|\n",
            "+---------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How could we access spark UI to check the metrics and tasks"
      ],
      "metadata": {
        "id": "cA4Kg25k2VlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -q pyngrok\n"
      ],
      "metadata": {
        "id": "QcBi6k1Xted6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Go to ngrok's sign-up page and create a free account.\n",
        "#You can sign up via gmail\n",
        "\n",
        "#https://dashboard.ngrok.com/login/mfa/setup\n",
        "\n",
        "#Then visit the dashboard\n",
        "#https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "\n"
      ],
      "metadata": {
        "id": "tKkK21j8uXVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ngrk](https://drive.google.com/uc?export=view&id=10OE6uTn2Ds5uFN_4CYjDG55zb64OwoZp)"
      ],
      "metadata": {
        "id": "72UQqTRGyDnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken XXXXXXXXXXXXXXXXX\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVgw6myzv51K",
        "outputId": "02b4b534-18ca-42c6-d5eb-c21ec0e64d24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Start the ngrok tunnel to the Spark UI\n",
        "public_url = ngrok.connect(4040)\n",
        "print(f\"Access Spark UI at: {public_url}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2cMQX5stigL",
        "outputId": "3263a253-da2e-49e2-fe3f-0322656be49a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2024-10-09T10:09:40+0000 lvl=warn msg=\"can't bind default web address, trying alternatives\" obj=web addr=127.0.0.1:4040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Access Spark UI at: NgrokTunnel: \"https://6bba-34-168-214-110.ngrok-free.app\" -> \"http://localhost:4040\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Thank you"
      ],
      "metadata": {
        "id": "6b1AtigH5Apk"
      }
    }
  ]
}